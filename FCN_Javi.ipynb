{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio as rio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.colors import ListedColormap\n",
    "from rasterio.enums import Resampling\n",
    "from PIL import ImageColor\n",
    "from skimage.exposure import rescale_intensity\n",
    "import rasterio as rio\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import ImageColor\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.layers import Conv2D, Conv2DTranspose, Dropout, MaxPooling2D, Input, concatenate\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from skimage.exposure import rescale_intensity\n",
    "from rasterio.enums import Resampling\n",
    "import rasterio.warp as warp\n",
    "\n",
    "import torch\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import MeanIoU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of data\n",
    "lc_dir = r'C:\\Users\\Usuari\\Documents\\TFM_Codigos\\MODELO\\data/7Labels.json'\n",
    "# Load Land Cover Parameter\n",
    "lc = json.load(open(lc_dir))\n",
    "lc_df = pd.DataFrame(lc)\n",
    "lc_df[\"values_normalize\"] = lc_df.index #+ 1\n",
    "lc_df[\"palette\"] = \"#\" + lc_df[\"palette\"]\n",
    "\n",
    "# Mapping from old to new values\n",
    "values = lc_df[\"values\"].to_list()\n",
    "values_norm = lc_df[\"values_normalize\"].to_list()\n",
    "palette = lc_df[\"palette\"].to_list()\n",
    "labels = lc_df[\"label\"].to_list()\n",
    "dict_values = {}\n",
    "dict_label = {}\n",
    "dict_palette = {}\n",
    "dict_palette_hex = {}\n",
    "for x in range(0, len(values)):\n",
    "    dict_values[values[x]] = values_norm[x]\n",
    "    dict_label[values_norm[x]] = labels[x]\n",
    "    dict_palette[values_norm[x]] = ImageColor.getrgb(palette[x])\n",
    "    dict_palette_hex[values_norm[x]] = palette[x]\n",
    "\n",
    "# Create colormap from values and palette\n",
    "cmap = ListedColormap(palette)\n",
    "\n",
    "# Patches legend\n",
    "patches = [\n",
    "    mpatches.Patch(color=palette[i], label=labels[i]) for i in range(len(values))\n",
    "]\n",
    "legend = {\n",
    "    \"handles\": patches,\n",
    "    \"bbox_to_anchor\": (1.05, 1),\n",
    "    \"loc\": 2,\n",
    "    \"borderaxespad\": 0.0,\n",
    "}\n",
    "lc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "def load_and_combine_datasets(folder_path):\n",
    "    # Listar todos los archivos en la carpeta\n",
    "    files = os.listdir(folder_path)\n",
    "    \n",
    "    # Expresiones regulares para identificar los archivos\n",
    "    image_train_pattern = re.compile(r\"images_train_PNOA_(\\d+)\\.npy\")\n",
    "    image_test_pattern = re.compile(r\"images_test_PNOA_(\\d+)\\.npy\")\n",
    "    lcs_train_pattern = re.compile(r\"lcs_train_PNOA_(\\d+)\\.npy\")\n",
    "    lcs_test_pattern = re.compile(r\"lcs_test_PNOA_(\\d+)\\.npy\")\n",
    "    \n",
    "    # Diccionarios para almacenar los arrays cargados\n",
    "    image_train_arrays = []\n",
    "    image_test_arrays = []\n",
    "    lcs_train_arrays = []\n",
    "    lcs_test_arrays = []\n",
    "    \n",
    "    # Recorrer todos los archivos en la carpeta\n",
    "    for file in files:\n",
    "        # Cargar y clasificar los archivos seg√∫n su tipo\n",
    "        if image_train_pattern.match(file):\n",
    "            image_train_arrays.append(np.load(os.path.join(folder_path, file)))\n",
    "        elif image_test_pattern.match(file):\n",
    "            image_test_arrays.append(np.load(os.path.join(folder_path, file)))\n",
    "        elif lcs_train_pattern.match(file):\n",
    "            lcs_train_arrays.append(np.load(os.path.join(folder_path, file)))\n",
    "        elif lcs_test_pattern.match(file):\n",
    "            lcs_test_arrays.append(np.load(os.path.join(folder_path, file)))\n",
    "    \n",
    "    # Combinar los arrays de cada categor√≠a\n",
    "    combined_images_train = np.concatenate(image_train_arrays, axis=0)\n",
    "    combined_images_test = np.concatenate(image_test_arrays, axis=0)\n",
    "    combined_lcs_train = np.concatenate(lcs_train_arrays, axis=0)\n",
    "    combined_lcs_test = np.concatenate(lcs_test_arrays, axis=0)\n",
    "    \n",
    "    return combined_images_train, combined_images_test, combined_lcs_train, combined_lcs_test\n",
    "\n",
    "# Uso de la funci√≥n\n",
    "folder_path = r\"C:\\Users\\Usuari\\Documents\\TFM_Codigos\\DATASET\\Data-arrays\"\n",
    "# Cambia esto por la ruta a tu carpeta\n",
    "images_train, images_test, lcs_train, lcs_test = load_and_combine_datasets(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train_predictors_shape: {images_train.shape}\\nTrain_label_shape: {lcs_train.shape}\\nTest_predictors_shape: {images_test.shape}\\nTest_label_shape: {lcs_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make lcs data into categorical\n",
    "# Keras model use different output data shape for category data\n",
    "# Convert it using utility from keras to make into categorical shape\n",
    "lcs_train_category = to_categorical(lcs_train)\n",
    "lcs_test_category = to_categorical(lcs_test)\n",
    "print(lcs_train_category.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el modelo DeepLab v3+ para im√°genes de 128x128\n",
    "def create_deeplabv3plus(input_shape, num_classes):\n",
    "    # Usar ResNet50V2 como backbone\n",
    "    base_model = tf.keras.applications.ResNet50V2(input_shape=input_shape, include_top=False, weights='imagenet')\n",
    "    \n",
    "    # Capas de la red base (ajustadas para 128x128)\n",
    "    layer_names = [\n",
    "        'conv2_block3_out',   # 1/4 de la resoluci√≥n original (32x32)\n",
    "        'conv3_block4_out',   # 1/8 de la resoluci√≥n original (16x16)\n",
    "        'conv4_block6_out',   # 1/16 de la resoluci√≥n original (8x8)\n",
    "    ]\n",
    "    layer_outputs = [base_model.get_layer(name).output for name in layer_names]\n",
    "    \n",
    "    # Crear el modelo de la red base\n",
    "    down_stack = tf.keras.Model(inputs=base_model.input, outputs=layer_outputs)\n",
    "    \n",
    "    # Congelar la red base\n",
    "    down_stack.trainable = False\n",
    "    \n",
    "    # Otra convoluci√≥n delpues del encoder  a tama√±o 4x4x256??? evitar sobreajuste con dropout\n",
    "    x = layers.Conv2D(256, (1, 1), padding='same', activation='relu')(down_stack.output[-2])  # 8√ó8 correcto\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "\n",
    "    # Decodificador corregido\n",
    "    x = layers.Conv2DTranspose(128, (3, 3), strides=(2, 2), padding='same', activation='relu')(x)  # 8√ó8 ‚Üí 16√ó16\n",
    "    x = layers.Conv2DTranspose(64, (3, 3), strides=(2, 2), padding='same', activation='relu')(x)   # 16√ó16 ‚Üí 32√ó32\n",
    "    x = layers.Conv2DTranspose(32, (3, 3), strides=(2, 2), padding='same', activation='relu')(x)   # 32√ó32 ‚Üí 64√ó64\n",
    "    x = layers.Conv2DTranspose(16, (3, 3), strides=(2, 2), padding='same', activation='relu')(x)   # 64√ó64 ‚Üí 128√ó128\n",
    "\n",
    "    # Salida final\n",
    "    x = layers.Conv2D(num_classes, (1, 1), padding='same', activation='softmax')(x)  # (128√ó128√ó7)\n",
    "\n",
    "    # Crear el modelo final\n",
    "    model = tf.keras.Model(inputs=down_stack.input, outputs=x)\n",
    "    for i, layer_output in enumerate(down_stack.outputs):\n",
    "        print(f\"Salida {i}: {layer_output.shape}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Par√°metros del modelo\n",
    "input_shape = (128, 128, 3)  # Tama√±o de las im√°genes\n",
    "num_classes = 7  # N√∫mero de clases\n",
    "\n",
    "# Crear el modelo\n",
    "model = create_deeplabv3plus(input_shape, num_classes)\n",
    "\n",
    "# Verificar la forma de salida del modelo\n",
    "dummy_input = tf.random.normal([1, 128, 128, 3])  # Entrada dummy de forma (1, 128, 128, 3)\n",
    "output = model(dummy_input)\n",
    "print(\"Forma de salida del modelo:\", output.shape)  # Deber√≠a ser (1, 128, 128, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import MeanIoU, CategoricalAccuracy, Precision, Recall, AUC\n",
    "# Par√°metros del modelo\n",
    "input_shape = (128, 128, 3)  # Tama√±o de las im√°genes\n",
    "num_classes = 7  # N√∫mero de clases\n",
    "\n",
    "# Crear el modelo\n",
    "model = create_deeplabv3plus(input_shape, num_classes)\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-4),\n",
    "    loss=CategoricalCrossentropy(),\n",
    "    metrics=[MeanIoU(num_classes=num_classes), CategoricalAccuracy(), Precision(), Recall(), AUC()]\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=5, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(factor=0.1, patience=5)\n",
    "]\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model.fit(\n",
    "    x=images_train,\n",
    "    y=lcs_train_category,\n",
    "    batch_size=16,  # Aumentar el batch size\n",
    "    epochs=35,\n",
    "    validation_data=(images_test, lcs_test_category),\n",
    "    callbacks=callbacks,\n",
    "    shuffle=True,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('FCN_rESnEt_v0.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# N√∫mero de im√°genes a mostrar\n",
    "num_samples = 10\n",
    "indices = np.random.choice(len(images_test), num_samples, replace=False)\n",
    "\n",
    "plt.figure(figsize=(12, num_samples * 3))\n",
    "\n",
    "for i, idx in enumerate(indices):\n",
    "    img = images_test[idx]\n",
    "    true_mask = lcs_test[idx]  # M√°scara real\n",
    "\n",
    "    # Predecir la m√°scara\n",
    "    pred_mask = model.predict(img[np.newaxis, ...])  # Agregar dimensi√≥n batch\n",
    "    pred_mask = np.argmax(pred_mask, axis=-1)[0]  # Convertir softmax a etiquetas de clase\n",
    "\n",
    "    # Mostrar la imagen original\n",
    "    plt.subplot(num_samples, 3, i * 3 + 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(\"Imagen original\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Mostrar la m√°scara real con los colores correctos\n",
    "    plt.subplot(num_samples, 3, i * 3 + 2)\n",
    "    plt.imshow(true_mask, cmap=cmap, vmin=0, vmax=len(palette) - 1)  # Usar el colormap definido\n",
    "    plt.title(\"M√°scara real\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Mostrar la predicci√≥n con los colores correctos\n",
    "    plt.subplot(num_samples, 3, i * 3 + 3)\n",
    "    plt.imshow(pred_mask, cmap=cmap, vmin=0, vmax=len(palette) - 1)  # Usar el colormap definido\n",
    "    plt.title(\"Predicci√≥n\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(2, 5))\n",
    "ax.legend(**legend)\n",
    "ax.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener los valores del historial con los nombres correctos\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "iou = history.history['mean_io_u_1']  # Nombre correcto\n",
    "val_iou = history.history['val_mean_io_u_1']  # Nombre correcto\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# üìâ Gr√°fico de P√©rdida\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, loss, 'bo-', label='Entrenamiento')\n",
    "plt.plot(epochs, val_loss, 'ro-', label='Validaci√≥n')\n",
    "plt.title('P√©rdida durante el entrenamiento')\n",
    "plt.xlabel('√âpocas')\n",
    "plt.ylabel('P√©rdida')\n",
    "plt.legend()\n",
    "\n",
    "# üìà Gr√°fico de MeanIoU\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, iou, 'bo-', label='Entrenamiento')\n",
    "plt.plot(epochs, val_iou, 'ro-', label='Validaci√≥n')\n",
    "plt.title('Mean IoU durante el entrenamiento')\n",
    "plt.xlabel('√âpocas')\n",
    "plt.ylabel('Mean IoU')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Obtener predicciones del modelo en el conjunto de prueba\n",
    "y_pred = model.predict(images_test)  # (N, 128, 128, num_classes)\n",
    "y_pred = np.argmax(y_pred, axis=-1)  # Convertir a etiquetas de clase (N, 128, 128)\n",
    "\n",
    "# Obtener las m√°scaras verdaderas (ground truth)\n",
    "y_true = np.argmax(lcs_test_category, axis=-1)  # Convertir one-hot a etiquetas (N, 128, 128)\n",
    "\n",
    "# Aplanar las m√°scaras para usar con sklearn (pixel a pixel)\n",
    "y_true_flat = y_true.flatten()\n",
    "y_pred_flat = y_pred.flatten()\n",
    "\n",
    "# Calcular precisi√≥n, recall y F1-score por clase\n",
    "precision = precision_score(y_true_flat, y_pred_flat, average=None)  # Por clase\n",
    "recall = recall_score(y_true_flat, y_pred_flat, average=None)  # Por clase\n",
    "f1 = f1_score(y_true_flat, y_pred_flat, average=None)  # Por clase\n",
    "\n",
    "# Mostrar los valores de F1-score por clase\n",
    "for i, f1_value in enumerate(f1):\n",
    "    print(f\"Clase {i}: F1-score = {f1_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Obtener las predicciones del modelo\n",
    "y_pred = model.predict(images_test)  # (N, 128, 128, num_classes)\n",
    "y_pred = np.argmax(y_pred, axis=-1)  # Convertir a etiquetas de clase (N, 128, 128)\n",
    "\n",
    "# Obtener las m√°scaras verdaderas (ground truth)\n",
    "y_true = np.argmax(lcs_test_category, axis=-1)  # Convertir one-hot a etiquetas (N, 128, 128)\n",
    "\n",
    "# Aplanar las predicciones y las etiquetas verdaderas para la matriz de confusi√≥n\n",
    "y_true_flat = y_true.flatten()\n",
    "y_pred_flat = y_pred.flatten()\n",
    "\n",
    "# Calcular la matriz de confusi√≥n\n",
    "cm = confusion_matrix(y_true_flat, y_pred_flat, labels=np.arange(num_classes))\n",
    "\n",
    "# Crear un gr√°fico con la matriz de confusi√≥n\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "plt.title('Matriz de Confusi√≥n')\n",
    "plt.xlabel('Predicci√≥n')\n",
    "plt.ylabel('Verdadero')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar la matriz de confusi√≥n (entre 0 y 1)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Visualizaci√≥n de la matriz normalizada\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues', xticklabels=labels, yticklabels=labels, cbar_kws={'label': 'Frecuencia Relativa'})\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "history_dict = history.history\n",
    "# Extraer las m√©tricas\n",
    "epochs = range(1, len(history_dict['loss']) + 1)\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "iou = history_dict.get('mean_io_u', None)  # Si tienes 'mean_io_u_1' como m√©trica\n",
    "val_iou = history_dict.get('val_mean_io_u', None)  # Similar para 'val_mean_io_u_1'\n",
    "\n",
    "# Crear un DataFrame\n",
    "data = {\n",
    "    'Epoch': epochs,\n",
    "    'Loss': loss,\n",
    "    'Validation Loss': val_loss,\n",
    "    'Mean IoU': iou,\n",
    "    'Validation Mean IoU': val_iou\n",
    "}\n",
    "\n",
    "# Convertir en un DataFrame de Pandas\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('training_metrics.csv', index=False)\n",
    "# Mostrar el DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un DataFrame con los resultados de F1 Score, Precision y Recall por clase\n",
    "f1_scores = pd.DataFrame({\n",
    "    'Class': labels,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1 Score': f1\n",
    "})\n",
    "\n",
    "# Guardar los F1 Scores en un archivo CSV\n",
    "f1_scores.to_csv('f1_scores.csv', index=False)\n",
    "\n",
    "# Mostrar el DataFrame con los resultados de F1 Score\n",
    "print(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show history\n",
    "history = pd.DataFrame(history.history)\n",
    "print(history)\n",
    "\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.plot(range(len(history['categorical_accuracy'].values.tolist())), history['categorical_accuracy'].values.tolist(), label = 'Train_Accuracy')\n",
    "plt.plot(range(len(history['loss'].values.tolist())), history['loss'].values.tolist(), label = 'Train_Loss')\n",
    "plt.plot(range(len(history['val_categorical_accuracy'].values.tolist())), history['val_categorical_accuracy'].values.tolist(), label = 'Test_Accuracy')\n",
    "plt.plot(range(len(history['val_loss'].values.tolist())), history['val_loss'].values.tolist(), label = 'Test_Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict test data\n",
    "prediction = np.argmax(model.predict(images_test), 3).flatten()\n",
    "label = lcs_test.flatten()\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(label, prediction, normalize='true')\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "cm = ConfusionMatrixDisplay(cm)\n",
    "cm.plot(ax = ax)\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(label, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
