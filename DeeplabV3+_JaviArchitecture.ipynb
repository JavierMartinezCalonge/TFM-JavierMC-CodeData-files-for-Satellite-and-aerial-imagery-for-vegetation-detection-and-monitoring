{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio as rio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.colors import ListedColormap\n",
    "from rasterio.enums import Resampling\n",
    "from PIL import ImageColor\n",
    "from skimage.exposure import rescale_intensity\n",
    "import rasterio as rio\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import ImageColor\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.layers import Conv2D, Conv2DTranspose, Dropout, MaxPooling2D, Input, concatenate\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from skimage.exposure import rescale_intensity\n",
    "from rasterio.enums import Resampling\n",
    "import rasterio.warp as warp\n",
    "\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import MeanIoU\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.metrics import MeanIoU, CategoricalAccuracy, Precision, Recall, AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of data\n",
    "lc_dir = r'C:\\Users\\Usuari\\Documents\\TFM_Codigos\\MODELO\\data/7Labels.json'\n",
    "# Load Land Cover Parameter\n",
    "lc = json.load(open(lc_dir))\n",
    "lc_df = pd.DataFrame(lc)\n",
    "lc_df[\"values_normalize\"] = lc_df.index #+ 1\n",
    "lc_df[\"palette\"] = \"#\" + lc_df[\"palette\"]\n",
    "\n",
    "# Mapping from old to new values\n",
    "values = lc_df[\"values\"].to_list()\n",
    "values_norm = lc_df[\"values_normalize\"].to_list()\n",
    "palette = lc_df[\"palette\"].to_list()\n",
    "labels = lc_df[\"label\"].to_list()\n",
    "dict_values = {}\n",
    "dict_label = {}\n",
    "dict_palette = {}\n",
    "dict_palette_hex = {}\n",
    "for x in range(0, len(values)):\n",
    "    dict_values[values[x]] = values_norm[x]\n",
    "    dict_label[values_norm[x]] = labels[x]\n",
    "    dict_palette[values_norm[x]] = ImageColor.getrgb(palette[x])\n",
    "    dict_palette_hex[values_norm[x]] = palette[x]\n",
    "\n",
    "# Create colormap from values and palette\n",
    "cmap = ListedColormap(palette)\n",
    "\n",
    "# Patches legend\n",
    "patches = [\n",
    "    mpatches.Patch(color=palette[i], label=labels[i]) for i in range(len(values))\n",
    "]\n",
    "legend = {\n",
    "    \"handles\": patches,\n",
    "    \"bbox_to_anchor\": (1.05, 1),\n",
    "    \"loc\": 2,\n",
    "    \"borderaxespad\": 0.0,\n",
    "}\n",
    "lc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "def load_and_combine_datasets(folder_path):\n",
    "    # Listar todos los archivos en la carpeta\n",
    "    files = os.listdir(folder_path)\n",
    "    \n",
    "    # Expresiones regulares para identificar los archivos\n",
    "    image_train_pattern = re.compile(r\"images_train_PNOA_(\\d+)\\.npy\")\n",
    "    image_test_pattern = re.compile(r\"images_test_PNOA_(\\d+)\\.npy\")\n",
    "    lcs_train_pattern = re.compile(r\"lcs_train_PNOA_(\\d+)\\.npy\")\n",
    "    lcs_test_pattern = re.compile(r\"lcs_test_PNOA_(\\d+)\\.npy\")\n",
    "    \n",
    "    # Diccionarios para almacenar los arrays cargados\n",
    "    image_train_arrays = []\n",
    "    image_test_arrays = []\n",
    "    lcs_train_arrays = []\n",
    "    lcs_test_arrays = []\n",
    "    \n",
    "    # Recorrer todos los archivos en la carpeta\n",
    "    for file in files:\n",
    "        # Cargar y clasificar los archivos seg煤n su tipo\n",
    "        if image_train_pattern.match(file):\n",
    "            image_train_arrays.append(np.load(os.path.join(folder_path, file)))\n",
    "        elif image_test_pattern.match(file):\n",
    "            image_test_arrays.append(np.load(os.path.join(folder_path, file)))\n",
    "        elif lcs_train_pattern.match(file):\n",
    "            lcs_train_arrays.append(np.load(os.path.join(folder_path, file)))\n",
    "        elif lcs_test_pattern.match(file):\n",
    "            lcs_test_arrays.append(np.load(os.path.join(folder_path, file)))\n",
    "    \n",
    "    # Combinar los arrays de cada categor铆a\n",
    "    combined_images_train = np.concatenate(image_train_arrays, axis=0)\n",
    "    combined_images_test = np.concatenate(image_test_arrays, axis=0)\n",
    "    combined_lcs_train = np.concatenate(lcs_train_arrays, axis=0)\n",
    "    combined_lcs_test = np.concatenate(lcs_test_arrays, axis=0)\n",
    "    \n",
    "    return combined_images_train, combined_images_test, combined_lcs_train, combined_lcs_test\n",
    "\n",
    "# Uso de la funci贸n\n",
    "folder_path = r\"C:\\Users\\Usuari\\Documents\\TFM_Codigos\\DATASET\\Data-arrays\"\n",
    "# Cambia esto por la ruta a tu carpeta\n",
    "images_train, images_test, lcs_train, lcs_test = load_and_combine_datasets(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train_predictors_shape: {images_train.shape}\\nTrain_label_shape: {lcs_train.shape}\\nTest_predictors_shape: {images_test.shape}\\nTest_label_shape: {lcs_test.shape}')\n",
    "# Make lcs data into categorical\n",
    "# Keras model use different output data shape for category data\n",
    "# Convert it using utility from keras to make into categorical shape\n",
    "lcs_train_category = to_categorical(lcs_train)\n",
    "lcs_test_category = to_categorical(lcs_test)\n",
    "print(lcs_train_category.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atrous_spatial_pyramid_pooling(x):\n",
    "    shape = tf.keras.backend.int_shape(x)\n",
    "    pool = layers.GlobalAveragePooling2D()(x)\n",
    "    pool = layers.Reshape((1, 1, shape[-1]))(pool)\n",
    "    pool = layers.Conv2D(256, (1, 1), padding=\"same\", activation=\"relu\")(pool)\n",
    "    pool = layers.UpSampling2D((shape[1], shape[2]), interpolation=\"bilinear\")(pool)\n",
    "\n",
    "    # Convoluciones con diferentes tasas de dilataci贸n\n",
    "    conv1 = layers.Conv2D(256, (1, 1), padding=\"same\", activation=\"relu\")(x)\n",
    "    conv6 = layers.Conv2D(256, (3, 3), padding=\"same\", dilation_rate=6, activation=\"relu\")(x)\n",
    "    conv12 = layers.Conv2D(256, (3, 3), padding=\"same\", dilation_rate=12, activation=\"relu\")(x)\n",
    "    conv18 = layers.Conv2D(256, (3, 3), padding=\"same\", dilation_rate=18, activation=\"relu\")(x)\n",
    "\n",
    "    # Concatenar todas las capas ASPP\n",
    "    x = layers.Concatenate()([pool, conv1, conv6, conv12, conv18])\n",
    "    x = layers.Conv2D(256, (1, 1), padding=\"same\", activation=\"relu\")(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def create_deeplabv3plus(input_shape, num_classes):\n",
    "    base_model = tf.keras.applications.MobileNetV2(input_shape=input_shape, include_top=False, weights=\"imagenet\")\n",
    "\n",
    "    # Extraer caracter铆sticas de inter茅s\n",
    "    layer_names = [\"block_1_expand_relu\", \"block_6_expand_relu\"]  \n",
    "    low_level_feature = base_model.get_layer(layer_names[0]).output  \n",
    "    high_level_feature = base_model.get_layer(layer_names[1]).output \n",
    "    base_model.trainable = False\n",
    "\n",
    "    # Aplicar ASPP en las caracter铆sticas de alta resoluci贸n\n",
    "    x = atrous_spatial_pyramid_pooling(high_level_feature)\n",
    "\n",
    "    # Upsampling para coincidir con las caracter铆sticas de baja resoluci贸n\n",
    "    x = layers.UpSampling2D((4, 4), interpolation=\"bilinear\")(x) \n",
    "\n",
    "    # Procesar caracter铆sticas de baja resoluci贸n\n",
    "    low_level_feature = layers.Conv2D(48, (1, 1), padding=\"same\", activation=\"relu\")(low_level_feature)\n",
    "\n",
    "    # Concatenar con ASPP\n",
    "    x = layers.Concatenate()([x, low_level_feature])\n",
    "\n",
    "    # Decodificador\n",
    "    x = layers.Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.UpSampling2D((2, 2), interpolation=\"bilinear\")(x)  \n",
    "    # Salida final\n",
    "    x = layers.Conv2D(num_classes, (1, 1), padding=\"same\", activation=\"softmax\")(x)\n",
    "\n",
    "    # Crear el modelo\n",
    "    model = tf.keras.Model(inputs=base_model.input, outputs=x)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Par谩metros del modelo\n",
    "input_shape = (128, 128, 3)\n",
    "num_classes = 7\n",
    "\n",
    "# Crear el modelo\n",
    "model = create_deeplabv3plus(input_shape, num_classes)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilar el modelo\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss=CategoricalCrossentropy(),\n",
    "    metrics=[MeanIoU(num_classes=num_classes), CategoricalAccuracy()]\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=5, restore_best_weights=True),\n",
    "]\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model.fit(\n",
    "    x=images_train,\n",
    "    y=lcs_train_category,\n",
    "    batch_size=16,\n",
    "    epochs=10,\n",
    "    validation_data=(images_test, lcs_test_category),\n",
    "    callbacks=callbacks,\n",
    "    shuffle=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('DeepLabV3_vabril.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# N煤mero de im谩genes a mostrar\n",
    "num_samples = 10\n",
    "indices = np.random.choice(len(images_test), num_samples, replace=False)\n",
    "\n",
    "plt.figure(figsize=(12, num_samples * 3))\n",
    "\n",
    "for i, idx in enumerate(indices):\n",
    "    img = images_test[idx]\n",
    "    true_mask = lcs_test[idx]  # M谩scara real\n",
    "\n",
    "    # Predecir la m谩scara\n",
    "    pred_mask = model.predict(img[np.newaxis, ...])  # Agregar dimensi贸n batch\n",
    "    pred_mask = np.argmax(pred_mask, axis=-1)[0]  # Convertir softmax a etiquetas de clase\n",
    "\n",
    "    # Mostrar la imagen original\n",
    "    plt.subplot(num_samples, 3, i * 3 + 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(\"Imagen original\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Mostrar la m谩scara real con los colores correctos\n",
    "    plt.subplot(num_samples, 3, i * 3 + 2)\n",
    "    plt.imshow(true_mask, cmap=cmap, vmin=0, vmax=len(palette) - 1)  # Usar el colormap definido\n",
    "    plt.title(\"M谩scara real\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Mostrar la predicci贸n con los colores correctos\n",
    "    plt.subplot(num_samples, 3, i * 3 + 3)\n",
    "    plt.imshow(pred_mask, cmap=cmap, vmin=0, vmax=len(palette) - 1)  # Usar el colormap definido\n",
    "    plt.title(\"Predicci贸n\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(2, 5))\n",
    "ax.legend(**legend)\n",
    "ax.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener los valores del historial con los nombres correctos\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "iou = history.history['mean_io_u']  # Nombre correcto\n",
    "val_iou = history.history['val_mean_io_u']  # Nombre correcto\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "#  Gr谩fico de P茅rdida\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, loss, 'bo-', label='Entrenamiento')\n",
    "plt.plot(epochs, val_loss, 'ro-', label='Validaci贸n')\n",
    "plt.title('P茅rdida durante el entrenamiento')\n",
    "plt.xlabel('pocas')\n",
    "plt.ylabel('P茅rdida')\n",
    "plt.legend()\n",
    "\n",
    "#  Gr谩fico de MeanIoU\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, iou, 'bo-', label='Entrenamiento')\n",
    "plt.plot(epochs, val_iou, 'ro-', label='Validaci贸n')\n",
    "plt.title('Mean IoU durante el entrenamiento')\n",
    "plt.xlabel('pocas')\n",
    "plt.ylabel('Mean IoU')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Obtener predicciones del modelo en el conjunto de prueba\n",
    "y_pred = model.predict(images_test)  # (N, 128, 128, num_classes)\n",
    "y_pred = np.argmax(y_pred, axis=-1)  # Convertir a etiquetas de clase (N, 128, 128)\n",
    "\n",
    "# Obtener las m谩scaras verdaderas (ground truth)\n",
    "y_true = np.argmax(lcs_test_category, axis=-1)  # Convertir one-hot a etiquetas (N, 128, 128)\n",
    "\n",
    "# Aplanar las m谩scaras para usar con sklearn (pixel a pixel)\n",
    "y_true_flat = y_true.flatten()\n",
    "y_pred_flat = y_pred.flatten()\n",
    "\n",
    "# Calcular precisi贸n, recall y F1-score por clase\n",
    "precision = precision_score(y_true_flat, y_pred_flat, average=None)  # Por clase\n",
    "recall = recall_score(y_true_flat, y_pred_flat, average=None)  # Por clase\n",
    "f1 = f1_score(y_true_flat, y_pred_flat, average=None)  # Por clase\n",
    "\n",
    "# Mostrar los valores de F1-score por clase\n",
    "for i, f1_value in enumerate(f1):\n",
    "    print(f\"Clase {i}: F1-score = {f1_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Obtener las predicciones del modelo\n",
    "y_pred = model.predict(images_test)  # (N, 128, 128, num_classes)\n",
    "y_pred = np.argmax(y_pred, axis=-1)  # Convertir a etiquetas de clase (N, 128, 128)\n",
    "\n",
    "# Obtener las m谩scaras verdaderas (ground truth)\n",
    "y_true = np.argmax(lcs_test_category, axis=-1)  # Convertir one-hot a etiquetas (N, 128, 128)\n",
    "\n",
    "# Aplanar las predicciones y las etiquetas verdaderas para la matriz de confusi贸n\n",
    "y_true_flat = y_true.flatten()\n",
    "y_pred_flat = y_pred.flatten()\n",
    "\n",
    "# Calcular la matriz de confusi贸n\n",
    "cm = confusion_matrix(y_true_flat, y_pred_flat, labels=np.arange(num_classes))\n",
    "\n",
    "# Crear un gr谩fico con la matriz de confusi贸n\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "plt.title('Matriz de Confusi贸n')\n",
    "plt.xlabel('Predicci贸n')\n",
    "plt.ylabel('Verdadero')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar la matriz de confusi贸n (entre 0 y 1)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Visualizaci贸n de la matriz normalizada\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues', xticklabels=labels, yticklabels=labels, cbar_kws={'label': 'Frecuencia Relativa'})\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "history_dict = history.history\n",
    "# Extraer las m茅tricas\n",
    "epochs = range(1, len(history_dict['loss']) + 1)\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "iou = history_dict.get('mean_io_u', None)  # Si tienes 'mean_io_u_2' como m茅trica\n",
    "val_iou = history_dict.get('val_mean_io_u', None)  # Similar para 'val_mean_io_u_2'\n",
    "\n",
    "# Crear un DataFrame\n",
    "data = {\n",
    "    'Epoch': epochs,\n",
    "    'Loss': loss,\n",
    "    'Validation Loss': val_loss,\n",
    "    'Mean IoU': iou,\n",
    "    'Validation Mean IoU': val_iou\n",
    "}\n",
    "\n",
    "# Convertir en un DataFrame de Pandas\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('training_metrics_d310epoch.csv', index=False)\n",
    "# Mostrar el DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un DataFrame con los resultados de F1 Score, Precision y Recall por clase\n",
    "f1_scores = pd.DataFrame({\n",
    "    'Class': labels,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1 Score': f1\n",
    "})\n",
    "\n",
    "# Guardar los F1 Scores en un archivo CSV\n",
    "f1_scores.to_csv('f1_scores_d310epoch.csv', index=False)\n",
    "\n",
    "# Mostrar el DataFrame con los resultados de F1 Score\n",
    "print(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show history\n",
    "history = pd.DataFrame(history.history)\n",
    "print(history)\n",
    "\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.plot(range(len(history['categorical_accuracy'].values.tolist())), history['categorical_accuracy'].values.tolist(), label = 'Train_Accuracy')\n",
    "plt.plot(range(len(history['loss'].values.tolist())), history['loss'].values.tolist(), label = 'Train_Loss')\n",
    "plt.plot(range(len(history['val_categorical_accuracy'].values.tolist())), history['val_categorical_accuracy'].values.tolist(), label = 'Test_Accuracy')\n",
    "plt.plot(range(len(history['val_loss'].values.tolist())), history['val_loss'].values.tolist(), label = 'Test_Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict test data\n",
    "prediction = np.argmax(model.predict(images_test), 3).flatten()\n",
    "label = lcs_test.flatten()\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(label, prediction, normalize='true')\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "cm = ConfusionMatrixDisplay(cm)\n",
    "cm.plot(ax = ax)\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(label, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_non_augment_size = int(len(images_test) / 8)\n",
    "\n",
    "plt.figure(figsize=(8, image_non_augment_size * 2))\n",
    "\n",
    "# Apply to classified image of the image\n",
    "for x in range(0, image_non_augment_size - 1):\n",
    "\tindex = x * 8\n",
    "\n",
    "\timage = images_test[index:(index + 1)]\n",
    "\tpred = model.predict(image)\n",
    "\tpred = np.argmax(pred, 3)[0]\n",
    "\n",
    "\tplt.subplot(image_non_augment_size, 3, x * 3 + 1)\n",
    "\tplt.imshow(image[0])\n",
    "\n",
    "\tplt.subplot(image_non_augment_size, 3, x * 3 + 2)\n",
    "\tplt.imshow(lcs_test[index], cmap=cmap, interpolation='nearest', vmin=0, vmax=6)\n",
    "\n",
    "\tplt.subplot(image_non_augment_size, 3, x * 3 + 3)\n",
    "\tplt.imshow(pred, cmap=cmap, interpolation='nearest', vmin=0, vmax=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
