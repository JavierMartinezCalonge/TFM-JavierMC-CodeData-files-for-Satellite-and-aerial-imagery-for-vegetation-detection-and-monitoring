{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio as rio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.colors import ListedColormap\n",
    "from rasterio.enums import Resampling\n",
    "from PIL import ImageColor\n",
    "from skimage.exposure import rescale_intensity\n",
    "import rasterio as rio\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import ImageColor\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.layers import Conv2D, Conv2DTranspose, Dropout, MaxPooling2D, Input, concatenate\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from skimage.exposure import rescale_intensity\n",
    "from rasterio.enums import Resampling\n",
    "import rasterio.warp as warp\n",
    "\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import MeanIoU\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.metrics import MeanIoU, CategoricalAccuracy, Precision, Recall, AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of data\n",
    "lc_dir = r'C:\\Users\\Usuari\\Documents\\TFM_Codigos\\MODELO\\data/7Labels.json'\n",
    "# Load Land Cover Parameter\n",
    "lc = json.load(open(lc_dir))\n",
    "lc_df = pd.DataFrame(lc)\n",
    "lc_df[\"values_normalize\"] = lc_df.index #+ 1\n",
    "lc_df[\"palette\"] = \"#\" + lc_df[\"palette\"]\n",
    "\n",
    "# Mapping from old to new values\n",
    "values = lc_df[\"values\"].to_list()\n",
    "values_norm = lc_df[\"values_normalize\"].to_list()\n",
    "palette = lc_df[\"palette\"].to_list()\n",
    "labels = lc_df[\"label\"].to_list()\n",
    "dict_values = {}\n",
    "dict_label = {}\n",
    "dict_palette = {}\n",
    "dict_palette_hex = {}\n",
    "for x in range(0, len(values)):\n",
    "    dict_values[values[x]] = values_norm[x]\n",
    "    dict_label[values_norm[x]] = labels[x]\n",
    "    dict_palette[values_norm[x]] = ImageColor.getrgb(palette[x])\n",
    "    dict_palette_hex[values_norm[x]] = palette[x]\n",
    "\n",
    "# Create colormap from values and palette\n",
    "cmap = ListedColormap(palette)\n",
    "\n",
    "# Patches legend\n",
    "patches = [\n",
    "    mpatches.Patch(color=palette[i], label=labels[i]) for i in range(len(values))\n",
    "]\n",
    "legend = {\n",
    "    \"handles\": patches,\n",
    "    \"bbox_to_anchor\": (1.05, 1),\n",
    "    \"loc\": 2,\n",
    "    \"borderaxespad\": 0.0,\n",
    "}\n",
    "lc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "def load_and_combine_datasets(folder_path):\n",
    "    # Listar todos los archivos en la carpeta\n",
    "    files = os.listdir(folder_path)\n",
    "    \n",
    "    # Expresiones regulares para identificar los archivos\n",
    "    image_train_pattern = re.compile(r\"images_train_PNOA_(\\d+)\\.npy\")\n",
    "    image_test_pattern = re.compile(r\"images_test_PNOA_(\\d+)\\.npy\")\n",
    "    lcs_train_pattern = re.compile(r\"lcs_train_PNOA_(\\d+)\\.npy\")\n",
    "    lcs_test_pattern = re.compile(r\"lcs_test_PNOA_(\\d+)\\.npy\")\n",
    "    \n",
    "    # Diccionarios para almacenar los arrays cargados\n",
    "    image_train_arrays = []\n",
    "    image_test_arrays = []\n",
    "    lcs_train_arrays = []\n",
    "    lcs_test_arrays = []\n",
    "    \n",
    "    # Recorrer todos los archivos en la carpeta\n",
    "    for file in files:\n",
    "        # Cargar y clasificar los archivos según su tipo\n",
    "        if image_train_pattern.match(file):\n",
    "            image_train_arrays.append(np.load(os.path.join(folder_path, file)))\n",
    "        elif image_test_pattern.match(file):\n",
    "            image_test_arrays.append(np.load(os.path.join(folder_path, file)))\n",
    "        elif lcs_train_pattern.match(file):\n",
    "            lcs_train_arrays.append(np.load(os.path.join(folder_path, file)))\n",
    "        elif lcs_test_pattern.match(file):\n",
    "            lcs_test_arrays.append(np.load(os.path.join(folder_path, file)))\n",
    "    \n",
    "    # Combinar los arrays de cada categoría\n",
    "    combined_images_train = np.concatenate(image_train_arrays, axis=0)\n",
    "    combined_images_test = np.concatenate(image_test_arrays, axis=0)\n",
    "    combined_lcs_train = np.concatenate(lcs_train_arrays, axis=0)\n",
    "    combined_lcs_test = np.concatenate(lcs_test_arrays, axis=0)\n",
    "    \n",
    "    return combined_images_train, combined_images_test, combined_lcs_train, combined_lcs_test\n",
    "\n",
    "# Uso de la función\n",
    "folder_path = r\"C:\\Users\\Usuari\\Documents\\TFM_Codigos\\DATASET\\Data-arrays\"\n",
    "# Cambia esto por la ruta a tu carpeta\n",
    "images_train, images_test, lcs_train, lcs_test = load_and_combine_datasets(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train_predictors_shape: {images_train.shape}\\nTrain_label_shape: {lcs_train.shape}\\nTest_predictors_shape: {images_test.shape}\\nTest_label_shape: {lcs_test.shape}')\n",
    "# Make lcs data into categorical\n",
    "# Keras model use different output data shape for category data\n",
    "# Convert it using utility from keras to make into categorical shape\n",
    "lcs_train_category = to_categorical(lcs_train)\n",
    "lcs_test_category = to_categorical(lcs_test)\n",
    "print(lcs_train_category.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atrous_spatial_pyramid_pooling(x):\n",
    "    shape = tf.keras.backend.int_shape(x)\n",
    "    pool = layers.GlobalAveragePooling2D()(x)\n",
    "    pool = layers.Reshape((1, 1, shape[-1]))(pool)\n",
    "    pool = layers.Conv2D(256, (1, 1), padding=\"same\", activation=\"relu\")(pool)\n",
    "    pool = layers.UpSampling2D((shape[1], shape[2]), interpolation=\"bilinear\")(pool)\n",
    "\n",
    "    # Convoluciones con diferentes tasas de dilatación\n",
    "    conv1 = layers.Conv2D(256, (1, 1), padding=\"same\", activation=\"relu\")(x)\n",
    "    conv6 = layers.Conv2D(256, (3, 3), padding=\"same\", dilation_rate=6, activation=\"relu\")(x)\n",
    "    conv12 = layers.Conv2D(256, (3, 3), padding=\"same\", dilation_rate=12, activation=\"relu\")(x)\n",
    "    conv18 = layers.Conv2D(256, (3, 3), padding=\"same\", dilation_rate=18, activation=\"relu\")(x)\n",
    "\n",
    "    # Concatenar todas las capas ASPP\n",
    "    x = layers.Concatenate()([pool, conv1, conv6, conv12, conv18])\n",
    "    x = layers.Conv2D(256, (1, 1), padding=\"same\", activation=\"relu\")(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def create_deeplabv3plus(input_shape, num_classes):\n",
    "    base_model = tf.keras.applications.MobileNetV2(input_shape=input_shape, include_top=False, weights=\"imagenet\")\n",
    "\n",
    "    # Extraer características de interés\n",
    "    layer_names = [\"block_1_expand_relu\", \"block_6_expand_relu\"]  \n",
    "    low_level_feature = base_model.get_layer(layer_names[0]).output  \n",
    "    high_level_feature = base_model.get_layer(layer_names[1]).output \n",
    "    base_model.trainable = False\n",
    "\n",
    "    # Aplicar ASPP en las características de alta resolución\n",
    "    x = atrous_spatial_pyramid_pooling(high_level_feature)\n",
    "\n",
    "    # Upsampling para coincidir con las características de baja resolución\n",
    "    x = layers.UpSampling2D((4, 4), interpolation=\"bilinear\")(x) \n",
    "\n",
    "    # Procesar características de baja resolución\n",
    "    low_level_feature = layers.Conv2D(48, (1, 1), padding=\"same\", activation=\"relu\")(low_level_feature)\n",
    "\n",
    "    # Concatenar con ASPP\n",
    "    x = layers.Concatenate()([x, low_level_feature])\n",
    "\n",
    "    # Decodificador\n",
    "    x = layers.Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.UpSampling2D((2, 2), interpolation=\"bilinear\")(x)  \n",
    "    # Salida final\n",
    "    x = layers.Conv2D(num_classes, (1, 1), padding=\"same\", activation=\"softmax\")(x)\n",
    "\n",
    "    # Crear el modelo\n",
    "    model = tf.keras.Model(inputs=base_model.input, outputs=x)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Parámetros del modelo\n",
    "input_shape = (128, 128, 3)\n",
    "num_classes = 7\n",
    "\n",
    "# Crear el modelo\n",
    "model = create_deeplabv3plus(input_shape, num_classes)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilar el modelo\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss=CategoricalCrossentropy(),\n",
    "    metrics=[MeanIoU(num_classes=num_classes), CategoricalAccuracy()]\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=5, restore_best_weights=True),\n",
    "]\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model.fit(\n",
    "    x=images_train,\n",
    "    y=lcs_train_category,\n",
    "    batch_size=16,\n",
    "    epochs=10,\n",
    "    validation_data=(images_test, lcs_test_category),\n",
    "    callbacks=callbacks,\n",
    "    shuffle=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('DeepLabV3_vabril.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Número de imágenes a mostrar\n",
    "num_samples = 10\n",
    "indices = np.random.choice(len(images_test), num_samples, replace=False)\n",
    "\n",
    "plt.figure(figsize=(12, num_samples * 3))\n",
    "\n",
    "for i, idx in enumerate(indices):\n",
    "    img = images_test[idx]\n",
    "    true_mask = lcs_test[idx]  # Máscara real\n",
    "\n",
    "    # Predecir la máscara\n",
    "    pred_mask = model.predict(img[np.newaxis, ...])  # Agregar dimensión batch\n",
    "    pred_mask = np.argmax(pred_mask, axis=-1)[0]  # Convertir softmax a etiquetas de clase\n",
    "\n",
    "    # Mostrar la imagen original\n",
    "    plt.subplot(num_samples, 3, i * 3 + 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(\"Imagen original\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Mostrar la máscara real con los colores correctos\n",
    "    plt.subplot(num_samples, 3, i * 3 + 2)\n",
    "    plt.imshow(true_mask, cmap=cmap, vmin=0, vmax=len(palette) - 1)  # Usar el colormap definido\n",
    "    plt.title(\"Máscara real\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Mostrar la predicción con los colores correctos\n",
    "    plt.subplot(num_samples, 3, i * 3 + 3)\n",
    "    plt.imshow(pred_mask, cmap=cmap, vmin=0, vmax=len(palette) - 1)  # Usar el colormap definido\n",
    "    plt.title(\"Predicción\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(2, 5))\n",
    "ax.legend(**legend)\n",
    "ax.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener los valores del historial con los nombres correctos\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "iou = history.history['mean_io_u']  # Nombre correcto\n",
    "val_iou = history.history['val_mean_io_u']  # Nombre correcto\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# 📉 Gráfico de Pérdida\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, loss, 'bo-', label='Entrenamiento')\n",
    "plt.plot(epochs, val_loss, 'ro-', label='Validación')\n",
    "plt.title('Pérdida durante el entrenamiento')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.legend()\n",
    "\n",
    "# 📈 Gráfico de MeanIoU\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, iou, 'bo-', label='Entrenamiento')\n",
    "plt.plot(epochs, val_iou, 'ro-', label='Validación')\n",
    "plt.title('Mean IoU durante el entrenamiento')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Mean IoU')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Obtener predicciones del modelo en el conjunto de prueba\n",
    "y_pred = model.predict(images_test)  # (N, 128, 128, num_classes)\n",
    "y_pred = np.argmax(y_pred, axis=-1)  # Convertir a etiquetas de clase (N, 128, 128)\n",
    "\n",
    "# Obtener las máscaras verdaderas (ground truth)\n",
    "y_true = np.argmax(lcs_test_category, axis=-1)  # Convertir one-hot a etiquetas (N, 128, 128)\n",
    "\n",
    "# Aplanar las máscaras para usar con sklearn (pixel a pixel)\n",
    "y_true_flat = y_true.flatten()\n",
    "y_pred_flat = y_pred.flatten()\n",
    "\n",
    "# Calcular precisión, recall y F1-score por clase\n",
    "precision = precision_score(y_true_flat, y_pred_flat, average=None)  # Por clase\n",
    "recall = recall_score(y_true_flat, y_pred_flat, average=None)  # Por clase\n",
    "f1 = f1_score(y_true_flat, y_pred_flat, average=None)  # Por clase\n",
    "\n",
    "# Mostrar los valores de F1-score por clase\n",
    "for i, f1_value in enumerate(f1):\n",
    "    print(f\"Clase {i}: F1-score = {f1_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Obtener las predicciones del modelo\n",
    "y_pred = model.predict(images_test)  # (N, 128, 128, num_classes)\n",
    "y_pred = np.argmax(y_pred, axis=-1)  # Convertir a etiquetas de clase (N, 128, 128)\n",
    "\n",
    "# Obtener las máscaras verdaderas (ground truth)\n",
    "y_true = np.argmax(lcs_test_category, axis=-1)  # Convertir one-hot a etiquetas (N, 128, 128)\n",
    "\n",
    "# Aplanar las predicciones y las etiquetas verdaderas para la matriz de confusión\n",
    "y_true_flat = y_true.flatten()\n",
    "y_pred_flat = y_pred.flatten()\n",
    "\n",
    "# Calcular la matriz de confusión\n",
    "cm = confusion_matrix(y_true_flat, y_pred_flat, labels=np.arange(num_classes))\n",
    "\n",
    "# Crear un gráfico con la matriz de confusión\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "plt.title('Matriz de Confusión')\n",
    "plt.xlabel('Predicción')\n",
    "plt.ylabel('Verdadero')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar la matriz de confusión (entre 0 y 1)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Visualización de la matriz normalizada\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues', xticklabels=labels, yticklabels=labels, cbar_kws={'label': 'Frecuencia Relativa'})\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "history_dict = history.history\n",
    "# Extraer las métricas\n",
    "epochs = range(1, len(history_dict['loss']) + 1)\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "iou = history_dict.get('mean_io_u', None)  # Si tienes 'mean_io_u_2' como métrica\n",
    "val_iou = history_dict.get('val_mean_io_u', None)  # Similar para 'val_mean_io_u_2'\n",
    "\n",
    "# Crear un DataFrame\n",
    "data = {\n",
    "    'Epoch': epochs,\n",
    "    'Loss': loss,\n",
    "    'Validation Loss': val_loss,\n",
    "    'Mean IoU': iou,\n",
    "    'Validation Mean IoU': val_iou\n",
    "}\n",
    "\n",
    "# Convertir en un DataFrame de Pandas\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('training_metrics_d310epoch.csv', index=False)\n",
    "# Mostrar el DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un DataFrame con los resultados de F1 Score, Precision y Recall por clase\n",
    "f1_scores = pd.DataFrame({\n",
    "    'Class': labels,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1 Score': f1\n",
    "})\n",
    "\n",
    "# Guardar los F1 Scores en un archivo CSV\n",
    "f1_scores.to_csv('f1_scores_d310epoch.csv', index=False)\n",
    "\n",
    "# Mostrar el DataFrame con los resultados de F1 Score\n",
    "print(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show history\n",
    "history = pd.DataFrame(history.history)\n",
    "print(history)\n",
    "\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.plot(range(len(history['categorical_accuracy'].values.tolist())), history['categorical_accuracy'].values.tolist(), label = 'Train_Accuracy')\n",
    "plt.plot(range(len(history['loss'].values.tolist())), history['loss'].values.tolist(), label = 'Train_Loss')\n",
    "plt.plot(range(len(history['val_categorical_accuracy'].values.tolist())), history['val_categorical_accuracy'].values.tolist(), label = 'Test_Accuracy')\n",
    "plt.plot(range(len(history['val_loss'].values.tolist())), history['val_loss'].values.tolist(), label = 'Test_Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict test data\n",
    "prediction = np.argmax(model.predict(images_test), 3).flatten()\n",
    "label = lcs_test.flatten()\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(label, prediction, normalize='true')\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "cm = ConfusionMatrixDisplay(cm)\n",
    "cm.plot(ax = ax)\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(label, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_non_augment_size = int(len(images_test) / 8)\n",
    "\n",
    "plt.figure(figsize=(8, image_non_augment_size * 2))\n",
    "\n",
    "# Apply to classified image of the image\n",
    "for x in range(0, image_non_augment_size - 1):\n",
    "\tindex = x * 8\n",
    "\n",
    "\timage = images_test[index:(index + 1)]\n",
    "\tpred = model.predict(image)\n",
    "\tpred = np.argmax(pred, 3)[0]\n",
    "\n",
    "\tplt.subplot(image_non_augment_size, 3, x * 3 + 1)\n",
    "\tplt.imshow(image[0])\n",
    "\n",
    "\tplt.subplot(image_non_augment_size, 3, x * 3 + 2)\n",
    "\tplt.imshow(lcs_test[index], cmap=cmap, interpolation='nearest', vmin=0, vmax=6)\n",
    "\n",
    "\tplt.subplot(image_non_augment_size, 3, x * 3 + 3)\n",
    "\tplt.imshow(pred, cmap=cmap, interpolation='nearest', vmin=0, vmax=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
